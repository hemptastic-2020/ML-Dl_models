{"cells":[{"metadata":{"_uuid":"dd4ba8058395bd3c8905074f9955f7e3ab30109a"},"cell_type":"markdown","source":"K-Nearest Neighbors  \nNaive Bayes  \nDecision Tree  \nRandom Forest  \nXgBoost  \nSupport Vector Machine  \nNeural Network"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n# read file\nvoice=pd.read_csv('../input/voicegender/voice.csv')\nvoice.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65467942b1dcc24d03e0ab21e8d037735c9a504a"},"cell_type":"code","source":"voice.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39e154cb54356acfac8c2edd1e565b42edf0b502"},"cell_type":"code","source":"voice.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5966f35aea13387065d525aabdcceaa809e08eeb"},"cell_type":"markdown","source":"Preprocessing: label encoder and normalization"},{"metadata":{"trusted":true,"_uuid":"523e9d19559457ef6269c8ff9f68a1fd15a6a3fe"},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nvoice[\"label\"] = le.fit_transform(voice[\"label\"])\nle.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dacdfb24e542b09bfacbf9dd9dfe497b630c02d1"},"cell_type":"code","source":"voice[:]=preprocessing.MinMaxScaler().fit_transform(voice)\nvoice","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"588a134786db193657fb77b66b1db729b6918dc5"},"cell_type":"markdown","source":"Visualization"},{"metadata":{"trusted":true,"_uuid":"1a6980e4e7cff1cf41c91df28d0163d30eaaf92a"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.subplots(4,5,figsize=(15,15))\nfor i in range(1,21):\n    plt.subplot(4,5,i)\n    plt.title(voice.columns[i-1])\n    sns.kdeplot(voice.loc[voice['label'] == 0, voice.columns[i-1]], color= 'green', label='F')\n    sns.kdeplot(voice.loc[voice['label'] == 1, voice.columns[i-1]], color= 'blue', label='M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21e6bee7bf4c73e4ded96b083658597cecd5490c"},"cell_type":"markdown","source":"At first glance, most significant features are Q25, IQR and meanfun. We will build models by using the 20 features and the 3 distinct features."},{"metadata":{"trusted":true,"_uuid":"103c4db3fb2021b7883f65e9a97499a31c92b754"},"cell_type":"markdown","source":"Using K-Nearest Neighbors, Naive Bayes, Decision Tree, Random Forest, XgBoost, Support Vector Machine, Neural Network to build models"},{"metadata":{"trusted":true,"_uuid":"62857eccdc345f3621d97ec92f97887ee07a3fdb"},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn import neighbors\nfrom sklearn import naive_bayes\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn import svm\nfrom sklearn import neural_network\nimport xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51f326d159e40eac6101da061535b117850a4db4"},"cell_type":"code","source":"# Split the data\ntrain, test = train_test_split(voice, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42dd77bdc5316b9c9c119ed1819081987c92160c"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c6142330ed1ac21db52dd832e8d14b4a51ec91"},"cell_type":"code","source":"x_train = train.iloc[:, :-1]\ny_train = train[\"label\"]\nx_test = test.iloc[:, :-1]\ny_test = test[\"label\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bd6aec294c52b30c4743002125da4e4e2c7a236"},"cell_type":"code","source":"x_train3 = train[[\"meanfun\",\"IQR\",\"Q25\"]]\ny_train3 = train[\"label\"]\nx_test3 = test[[\"meanfun\",\"IQR\",\"Q25\"]]\ny_test3 = test[\"label\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06e2d94c86bbb87fbb404f41199af473562dd3ce"},"cell_type":"code","source":"def classify(model,x_train,y_train,x_test,y_test):\n    from sklearn.metrics import classification_report\n    target_names = ['female', 'male']\n    model.fit(x_train,y_train)\n    y_pred=model.predict(x_test)\n    print(classification_report(y_test, y_pred, target_names=target_names, digits=4))\n    a = metrics.accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\",a)\n    return a\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \naccli=pd.read_csv('../input/accuracy-list/Accuracy_List.csv')\naccli.set_index(\"Algorithm\", inplace = True)\naccli.at['K-Nearest Neighbours', 'Accuracy (considering only IQR, meanfun & Q25)'] = 0\naccli.at['K-Nearest Neighbours', 'Accuracy (considering all attributes)'] = 0\naccli.at['Na誰ve Bayes', 'Accuracy (considering only IQR, meanfun & Q25)'] = 0\naccli.at['Na誰ve Bayes', 'Accuracy (considering all attributes)'] = 0\naccli.at['Decision Tree', 'Accuracy (considering only IQR, meanfun & Q25)'] = 0\naccli.at['Decision Tree', 'Accuracy (considering all attributes)'] = 0\naccli.at['Random Forest', 'Accuracy (considering only IQR, meanfun & Q25)'] = 0\naccli.at['Random Forest', 'Accuracy (considering all attributes)'] = 0\naccli.at['XgBoost', 'Accuracy (considering only IQR, meanfun & Q25)'] = 0\naccli.at['XgBoost', 'Accuracy (considering all attributes)'] = 0\naccli.at['Support Vector Machine', 'Accuracy (considering only IQR, meanfun & Q25)'] = 0\naccli.at['Support Vector Machine', 'Accuracy (considering all attributes)'] = 0\naccli.at['Neural Network (MLP Classifier)', 'Accuracy (considering only IQR, meanfun & Q25)'] = 0\naccli.at['Neural Network (MLP Classifier)', 'Accuracy (considering all attributes)'] = 0\n\naccli\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list = accli.index\nlist","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1245212f815e4854a590e8e81f8775e220553fcc"},"cell_type":"markdown","source":"## K-Nearest Neighbors\nUsing neighbors.KNeighborsClassifier() to build the model."},{"metadata":{"trusted":true,"_uuid":"c28957bd07a357a385f7b3fb5c5c6630e5b8ef51"},"cell_type":"code","source":"def knn_error(k,x_train,y_train,x_test,y_test):\n    error_rate = []\n    K=range(1,k)\n    for i in K:\n        knn = neighbors.KNeighborsClassifier(n_neighbors = i)\n        knn.fit(x_train, y_train)\n        y_pred = knn.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    kloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at k=%s.\" % (error_rate[kloc], K[kloc]))\n\n    plt.plot(K, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. K Value')\n    plt.xlabel('K')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return K[kloc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12db95c14b8359ef21411fa3bae4f8e36910b390"},"cell_type":"code","source":"k=knn_error(21,x_train,y_train,x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04045f8adb7d4f11bb105ae2339d411042944120"},"cell_type":"code","source":"model = neighbors.KNeighborsClassifier(n_neighbors = k)\nt = classify(model,x_train,y_train,x_test,y_test)\naccli.at['K-Nearest Neighbours', 'Accuracy (considering all attributes)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ff5ae616d921334b6078b269fce5315e6aff198"},"cell_type":"code","source":"k=knn_error(21,x_train3,y_train3,x_test3,y_test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6673b594780364603ab4b2c7569e89fd2d300fa"},"cell_type":"code","source":"model = neighbors.KNeighborsClassifier(n_neighbors = k)\nt = classify(model,x_train3,y_train3,x_test3,y_test3)\naccli.at['K-Nearest Neighbours', 'Accuracy (considering only IQR, meanfun & Q25)'] = t","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7230fd72a64c9d5670906ec3c314b81c9e5b1c27"},"cell_type":"markdown","source":"## Naive Bayes\nUsing naive_bayes.GaussianNB() to build the model."},{"metadata":{"trusted":true,"_uuid":"8beb4625e6551e30c6a27f0beab11901515dc892"},"cell_type":"code","source":"model=naive_bayes.GaussianNB()\nt = classify(model,x_train,y_train,x_test,y_test)\naccli.at['Na誰ve Bayes', 'Accuracy (considering all attributes)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71535d01e7eedb6d106edcb9fef4806a84f771ec"},"cell_type":"code","source":"model=naive_bayes.GaussianNB()\nt = classify(model,x_train3,y_train3,x_test3,y_test3)\naccli.at['Na誰ve Bayes', 'Accuracy (considering only IQR, meanfun & Q25)'] = t","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a67d4259bc76a4a773fa1edef2de80cca35cd364"},"cell_type":"markdown","source":"## Decision Tree\nUsing tree.DecisionTreeClassifier() to build the model."},{"metadata":{"trusted":true,"_uuid":"a4a8ad5d33e2ee2c10ae0cf4196a3698cc888db5"},"cell_type":"code","source":"#Find the best parameter to prune the tree\ndef dt_error(n,x_train,y_train,x_test,y_test):\n    nodes = range(2, n)\n    error_rate = []\n    for k in nodes:\n        model = tree.DecisionTreeClassifier(max_leaf_nodes=k)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    kloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at n=%s.\" % (error_rate[kloc], nodes[kloc]))\n    plt.plot(nodes, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.xlabel('Tree Size')\n    plt.ylabel('Cross-Validated MSE')\n    plt.show()\n    return nodes[kloc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af8229e428283c3e6254c8c3ffc74a717f48f248"},"cell_type":"code","source":"n=dt_error(10,x_train,y_train,x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54c055d4acd416dae177ee7475aa5603cf0e9c50"},"cell_type":"code","source":"#prune tree\npruned_tree = tree.DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = n)\nt = classify(pruned_tree,x_train,y_train,x_test,y_test)\naccli.at['Decision Tree', 'Accuracy (considering all attributes)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d952f002e7a8f332b5981ba536d245d8fb5f996"},"cell_type":"code","source":"n=dt_error(15,x_train3,y_train3,x_test3,y_test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da7411ddc1ddfa28b78abffbb482bb146c521ce5"},"cell_type":"code","source":"#prune tree\npruned_tree = tree.DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = n)\nt = classify(pruned_tree,x_train3,y_train3,x_test3,y_test3)\naccli.at['Decision Tree', 'Accuracy (considering only IQR, meanfun & Q25)'] = t","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46a4988ac7ad003d816312ecfd64f00b8c67c039"},"cell_type":"markdown","source":"## Random Forest\nUsing ensemble.RandomForestClassifier() to build the model."},{"metadata":{"trusted":true,"_uuid":"2fc71fce9ffd6cb0ab72c121f64c58543d4a0e46"},"cell_type":"code","source":"def rf_error(n,x_train,y_train,x_test,y_test):\n    error_rate = []\n    e=range(1,n,20)\n    for i in e:\n        model = ensemble.RandomForestClassifier(n_estimators = i)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    nloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at n=%s.\" % (error_rate[nloc], e[nloc]))\n\n    plt.plot(e, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. n Value')\n    plt.xlabel('n')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return e[nloc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"089b35675746b29a608ccacb4fcfd4f927b289ff"},"cell_type":"code","source":"e=rf_error(100,x_train,y_train,x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aab2b9d4f745267e4e6750743c85087a8d21768"},"cell_type":"code","source":"model=ensemble.RandomForestClassifier(n_estimators = e)\nt = classify(model,x_train,y_train,x_test,y_test)\naccli.at['Random Forest', 'Accuracy (considering all attributes)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c8c329facf1f45557324e65e33972f8ad5fe163"},"cell_type":"code","source":"e=rf_error(100,x_train3,y_train3,x_test3,y_test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bb4b71e9ddd155114b8a57f053d8ec49cc38d8f"},"cell_type":"code","source":"model=ensemble.RandomForestClassifier(n_estimators = e)\nt = classify(model,x_train3,y_train3,x_test3,y_test3)\naccli.at['Random Forest', 'Accuracy (considering only IQR, meanfun & Q25)'] = t","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd8997fc20c7d47ebf1dd0acd2099c54c35ab90f"},"cell_type":"markdown","source":"## XgBoost\nUsing xgboost.XGBClassifier() to build the model."},{"metadata":{"trusted":true,"_uuid":"8f620ed8ffd579ae81ae0c192c5ad237d3231d90"},"cell_type":"code","source":"model = xgboost.XGBClassifier()\nt = classify(model,x_train,y_train,x_test,y_test)\naccli.at['XgBoost', 'Accuracy (considering all attributes)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ddab8a4cb2ca7f0b01982d648e475b197b134dd"},"cell_type":"code","source":"model = xgboost.XGBClassifier()\nt = classify(model,x_train3,y_train3,x_test3,y_test3)\naccli.at['XgBoost', 'Accuracy (considering only IQR, meanfun & Q25)'] = t","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34c7370162c458860e4482c5603251a1beea8a79"},"cell_type":"markdown","source":"## Support Vector Machine\nUsing svm.SVC() to build the model."},{"metadata":{"trusted":true,"_uuid":"08f613ebc0246c91fcbc3bac0d5c3d98f926fd53"},"cell_type":"code","source":"def svm_kernel(x_train,y_train,x_test,y_test):\n    rate=[]\n    kernel=['rbf','poly','linear']\n    for i in kernel:\n        model=svm.SVC(kernel=i).fit(x_train,y_train)\n        y_pred=model.predict(x_train)\n        print(i, ' in-sample accuracy in SVM: ', accuracy_score(y_train,y_pred))\n        y_pred=model.predict(x_test)\n        print(i, ' out-of-sample accuracy in SVM: ', accuracy_score(y_test,y_pred))\n        rate.append(accuracy_score(y_test,y_pred))\n    nloc = rate.index(max(rate))\n    print(\"Highest accuracy is %s occurs at %s kernel.\" % (rate[nloc], kernel[nloc]))\n    return kernel[nloc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24e66b1e5dfa56f7412c8838cbe8a3c8adc715b2"},"cell_type":"code","source":"def svm_error(k,C,x_train,y_train,x_test,y_test):\n    error_rate = []\n    C=range(1,C)\n    for i in C:\n        model=svm.SVC(kernel=k,C=i).fit(x_train,y_train)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    cloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at C=%s.\" % (error_rate[cloc], C[cloc]))\n\n    plt.plot(C, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. C Value')\n    plt.xlabel('C')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return C[cloc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44e8e2489c6fa478855fc0b0bd51cf974b5bda95"},"cell_type":"code","source":"k=svm_kernel(x_train,y_train,x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f4eeafdcc4adb58e3c1b0e36f8aad6cdfa8c5bb"},"cell_type":"code","source":"c=svm_error(k,10,x_train,y_train,x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b57af1c5acc2d6bb749a9f12d0f78929be0baa6c"},"cell_type":"code","source":"model=svm.SVC(kernel=k,C=c)\nt = classify(model,x_train,y_train,x_test,y_test)\naccli.at['Support Vector Machine', 'Accuracy (considering all attributes)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83338e6607af41d53d272376d0295ba399d46ab9"},"cell_type":"code","source":"k=svm_kernel(x_train3,y_train3,x_test3,y_test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6e5c9857c5c254adcc3db090add93286c5e6eb5"},"cell_type":"code","source":"c=svm_error(k,10,x_train3,y_train3,x_test3,y_test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd6da99232ae6ef54464deb33e97693667c2ca73"},"cell_type":"code","source":"model=svm.SVC(kernel=k,C=c)\nt = classify(model,x_train3,y_train3,x_test3,y_test3)\naccli.at['Support Vector Machine', 'Accuracy (considering only IQR, meanfun & Q25)'] = t\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c23858a034bc9f92745dbf689aa8a975999581f1"},"cell_type":"markdown","source":"## Neural Network\nUsing neural_network.MLPClassifier to build the model."},{"metadata":{"trusted":true,"_uuid":"2da26483ae2626a956fb99a724ee1e23975bc800"},"cell_type":"code","source":"def nn_error(n,x_train,y_train,x_test,y_test):\n    error_rate = []\n    hidden_layer=range(1,n)\n    for i in hidden_layer:\n        model = neural_network.MLPClassifier(solver='adam', alpha=1e-5,\n                                       hidden_layer_sizes=i,\n                                       activation='logistic',random_state=17,\n                                       max_iter=2000)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    kloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at C=%s.\" % (error_rate[kloc], hidden_layer[kloc]))\n\n    plt.plot(hidden_layer, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. Hidden Layer Size')\n    plt.xlabel('Size')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return hidden_layer[kloc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b20c9304c6d87c194020471ae3c7af05c3cfee97"},"cell_type":"code","source":"h=nn_error(20,x_train,y_train,x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71872aedd78f1b82bb97b01fe01d69cee119fc67"},"cell_type":"code","source":"model = neural_network.MLPClassifier(solver='adam', alpha=1e-5,\n                                       hidden_layer_sizes=h,\n                                       activation='logistic',random_state=17,\n                                       max_iter=2000)\nt = classify(model,x_train,y_train,x_test,y_test)\naccli.at['Neural Network (MLP Classifier)', 'Accuracy (considering all attributes)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2bae0868755af334360d923b59dc57723eaadca"},"cell_type":"code","source":"h=nn_error(20,x_train3,y_train3,x_test3,y_test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"371a5ed6e7b31ca3caf282e93b641e39b0f18e2d"},"cell_type":"code","source":"model = neural_network.MLPClassifier(solver='adam', alpha=1e-5,\n                                       hidden_layer_sizes=h,\n                                       activation='logistic',random_state=17,\n                                       max_iter=2000)\nt = classify(model,x_train3,y_train3,x_test3,y_test3)\naccli.at['Neural Network (MLP Classifier)', 'Accuracy (considering only IQR, meanfun & Q25)'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accli","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accli.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accli['Accuracy (considering only IQR, meanfun & Q25)'].tolist()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accli['Accuracy (considering all attributes)'].tolist()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt \n  \n# creating dataframe \naccli2 = pd.DataFrame({ \n    'Algorithm': ['K-Nearest Neighbours', 'Na誰ve Bayes', 'Decision Tree', 'Random Forest',\n       'XgBoost', 'Support Vector Machine', 'Neural Network (MLP Classifier)'], \n    'Accuracy (considering only IQR, meanfun & Q25)': accli['Accuracy (considering only IQR, meanfun & Q25)'].tolist(), \n    'Accuracy (considering all attributes)': accli['Accuracy (considering all attributes)'].tolist() \n}) \n  \n# plotting graph \naccli2.plot(x=\"Algorithm\", y=[\"Accuracy (considering only IQR, meanfun & Q25)\", \"Accuracy (considering all attributes)\"],ylim=(0.84,1.00),figsize=(14,10),title='Accuracy_Score vs Algorithms', kind=\"bar\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Hence, the XgBoost algorithm gives the highest accuracy rate.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}